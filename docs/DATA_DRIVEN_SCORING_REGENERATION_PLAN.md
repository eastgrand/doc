# Data-Driven Scoring Algorithm Regeneration Plan

## Overview

This document outlines the plan to create a **completely data-driven scoring system** that regenerates scoring algorithms based on microservice SHAP feature importance analysis. This system is designed to be **recreated with every new project/data change** to ensure scoring algorithms reflect actual data relationships rather than outdated business assumptions.

## Problem Statement

### Current Issues
- **Outdated algorithms**: Hardcoded weights (35%, 30%, 20%, 15%) may not reflect current data patterns
- **Manual maintenance**: Each new project requires manual algorithm updates
- **Subjective scoring**: Business-defined weights may not align with actual data relationships
- **Inconsistent approaches**: Different scoring scripts use different methodologies

### Solution Approach
**Data-Driven Scoring**: Use microservice SHAP feature importance to automatically generate scoring algorithms that reflect actual statistical relationships in the data.

## Architecture

### **Data-Driven Algorithm Generation Pipeline**

```
Existing Endpoint Data â†’ SHAP Feature Extraction â†’ Importance Analysis â†’ Algorithm Generation â†’ Scoring Script Creation
         â†“                       â†“                       â†“                    â†“                        â†“
    Endpoint JSON files    Extract from nike_factor   Rank by importance   Mathematical formulas    JavaScript files
    (/public/data/endpoints)    importance dataset    for each analysis    with data-driven weights  ready to execute
         â†“                       â†“                       â†“                    â†“                        â†“
    CURRENT SOURCE:         Target: MP30034A_B_P    Top factors by SHAP   Field-specific weights    Replace current scripts
    feature-importance-      Model accuracy: 99.97%   importance values    based on actual data      with data-driven ones
    ranking.json            30 ranked factors                                                         
```

**Key Discovery**: Feature importance data already exists in:

- `public/data/microservice-export.json` â†’ `nike_factor_importance` dataset (**EXAMPLE STRUCTURE**)
- `public/data/endpoints/feature-importance-ranking.json` â†’ Complete endpoint data
- **EXAMPLE** Target variable: `MP30034A_B_P` (Nike market share from existing data)
- **EXAMPLE** Model accuracy: 99.97% (highly reliable SHAP values from Nike project)
- 30 ranked factors with SHAP importance scores (**template for any project**)

### **Per-Project Regeneration Workflow**

```
New Project Data â†’ Train Microservice â†’ Extract SHAP â†’ Generate Algorithms â†’ Create Scoring Scripts â†’ Execute Scoring
      â†“                    â†“               â†“               â†“                    â†“                   â†“
   New dataset       New ML models    New importance   New formulas         New .js files      New scores
   Different fields   Different SHAP   Different ranks  Different weights    Project-specific   Data-aligned
```

## Implementation Plan

### **Phase 1: Local SHAP Data Extraction System**

#### Step 1: Local Endpoint SHAP Data Extraction
```python
# scripts/scoring/generators/shap_extractor.py
class LocalSHAPExtractor:
    """Extract and analyze SHAP feature importance from existing endpoint data"""
    
    def extract_from_endpoints(self, endpoints_dir: str = "public/data/endpoints"):
        """Extract SHAP values from local endpoint files"""
        # Load feature-importance-ranking.json
        # Extract importance_score and feature_importance_score fields
        # Returns ranked list of features with importance scores
        
    def extract_from_microservice_export(self, export_path: str = "public/data/microservice-export.json"):
        """Extract SHAP from nike_factor_importance dataset (EXAMPLE DATA)"""
        # EXAMPLE: nike_factor_importance with target MP30034A_B_P (Nike)
        # NOTE: This is example data structure, not current project targets
        # Returns feature importance patterns for algorithm generation
        
    def rank_features_by_analysis(self):
        """Generate feature rankings for each analysis type from local data"""
        return {
            'strategic': self.get_top_features('strategic', limit=6),
            'competitive': self.get_top_features('competitive', limit=5),
            'demographic': self.get_top_features('demographic', limit=8),
            'correlation': self.get_top_features('correlation', limit=4),
            'brand_analysis': self.get_top_features('brand_analysis', limit=5),
            # ... for all 15+ analysis types
        }
```

#### Step 2: Local Feature Importance Analysis
```python
# scripts/scoring/generators/importance_analyzer.py
def analyze_local_shap_importance(endpoints_dir: str):
    """Analyze SHAP importance from local endpoint data"""
    
    # Load local endpoint data with SHAP fields
    endpoint_data = load_endpoint_files(endpoints_dir)
    
    importance_matrix = {
        'analysis_type': {
            'feature_name': importance_score,
            'normalized_weight': percentage,
            'confidence': statistical_confidence
        }
    }
    
    # EXAMPLE output structure (using Nike data as template):
    # NOTE: Actual field names and target variables will depend on current project
    # {
    #   'strategic': {
    #     'target_variable': 0.342,        # 34.2% importance (project-specific)
    #     'median_income_field': 0.289,    # 28.9% importance  
    #     'population_field': 0.201,       # 20.1% importance
    #     'demographic_score': 0.168       # 16.8% importance
    #   }
    # }
    
    return extract_importance_patterns(endpoint_data)
```

### **Phase 2: Algorithm Generation Engine**

#### Step 3: Mathematical Formula Generator
```python
# scripts/scoring/generators/formula_generator.py
class DataDrivenFormulaGenerator:
    """Generate mathematical formulas based on SHAP importance"""
    
    def generate_formula(self, analysis_type: str, feature_importance: dict):
        """Create scoring formula from feature importance"""
        
        # Normalize weights to sum to 1.0
        normalized_weights = self.normalize_weights(feature_importance)
        
        # Generate mathematical formula
        formula = self.create_weighted_formula(normalized_weights)
        
        # Example output:
        # "strategic_score = (0.342 Ã— mp30034a_b_p_normalized) + 
        #                   (0.289 Ã— median_income_normalized) + 
        #                   (0.201 Ã— total_population_normalized) + 
        #                   (0.168 Ã— demographic_score_normalized)"
        
        return {
            'formula': formula,
            'weights': normalized_weights,
            'field_mappings': self.get_field_mappings(analysis_type),
            'normalization_ranges': self.calculate_normalization_ranges()
        }
```

#### Step 4: JavaScript Code Generator
```python
# scripts/scoring/generators/js_generator.py
class JavaScriptScoringGenerator:
    """Generate complete JavaScript scoring scripts"""
    
    def generate_scoring_script(self, analysis_type: str, formula_config: dict):
        """Create complete .js scoring script file"""
        
        template = self.load_template('scoring_script_template.js')
        
        # Fill template with:
        # - Analysis-specific logic
        # - Data-driven weights from SHAP
        # - Field normalization functions
        # - Error handling and validation
        # - Output formatting
        
        return self.render_template(template, {
            'analysis_type': analysis_type,
            'weights': formula_config['weights'],
            'fields': formula_config['field_mappings'],
            'normalization': formula_config['normalization_ranges']
        })
```

### **Phase 3: Automated Script Generation**

#### Step 5: Complete Pipeline Orchestrator
```python
# scripts/scoring/generators/regenerate_all_scoring.py
class ScoringRegenerator:
    """Orchestrate complete scoring algorithm regeneration"""
    
    def regenerate_for_project(self, endpoints_dir: str, project_name: str):
        """Complete regeneration workflow using local endpoint data"""
        
        print(f"ðŸ”„ Regenerating scoring algorithms for {project_name}...")
        
        # 1. Extract SHAP importance from local endpoint files
        shap_data = LocalSHAPExtractor().extract_from_endpoints(endpoints_dir)
        
        # 2. Analyze and rank feature importance from local data
        importance_matrix = ImportanceAnalyzer().analyze_local_shap_importance(shap_data)
        
        # 3. Generate mathematical formulas for any project's target variables
        formulas = {}
        for analysis_type in self.get_analysis_types():
            formulas[analysis_type] = FormulaGenerator().generate_formula(
                analysis_type, 
                importance_matrix[analysis_type]
            )
        
        # 4. Create JavaScript scoring scripts with project-specific fields
        for analysis_type, formula in formulas.items():
            script_content = JavaScriptGenerator().generate_scoring_script(
                analysis_type, 
                formula
            )
            
            # Write to file
            output_path = f"scripts/scoring/{analysis_type}-scores.js"
            self.write_script_file(output_path, script_content)
            
        print(f"âœ… Generated {len(formulas)} scoring scripts")
        return formulas

# Usage for any project:
# python regenerate_all_scoring.py --endpoints public/data/endpoints --project hrb_v3
```

### **Phase 4: Validation and Testing**

#### Step 6: Algorithm Validation System
```python
# scripts/scoring/generators/validator.py
class ScoringValidator:
    """Validate generated scoring algorithms"""
    
    def validate_algorithms(self, generated_scripts: list):
        """Test generated scoring scripts"""
        
        validation_results = {}
        
        for script_path in generated_scripts:
            # 1. Syntax validation
            syntax_valid = self.validate_javascript_syntax(script_path)
            
            # 2. Mathematical validation
            math_valid = self.validate_mathematical_logic(script_path)
            
            # 3. Data compatibility validation
            data_valid = self.test_with_sample_data(script_path)
            
            # 4. Score range validation (0-100)
            range_valid = self.validate_score_ranges(script_path)
            
            validation_results[script_path] = {
                'syntax': syntax_valid,
                'mathematics': math_valid,
                'data_compatibility': data_valid,
                'score_ranges': range_valid,
                'overall': all([syntax_valid, math_valid, data_valid, range_valid])
            }
            
        return validation_results
```

## Analysis Types to Regenerate

### **Core Analysis Types (15+)**
1. **strategic-value-scores.js** - Market opportunity and strategic positioning
2. **competitive-analysis-scores.js** - Brand competition and market share analysis
3. **demographic-opportunity-scores.js** - Population and demographic insights
4. **correlation-strength-scores.js** - Statistical correlations and relationships
5. **brand-analysis-scores.js** - Multi-brand performance comparison
6. **market-sizing-scores.js** - Market potential and opportunity sizing
7. **trend-strength-scores.js** - Temporal patterns and growth analysis
8. **anomaly-detection-scores.js** - Statistical outlier identification
9. **feature-interaction-scores.js** - Multi-variable relationship analysis
10. **outlier-detection-scores.js** - Exceptional market characteristics
11. **segment-profiling-scores.js** - Customer segmentation analysis
12. **scenario-analysis-scores.js** - What-if scenario modeling
13. **predictive-modeling-scores.js** - Forecasting and prediction accuracy
14. **spatial-cluster-scores.js** - Geographic clustering analysis
15. **feature-importance-scores.js** - Variable importance ranking

### **Each Script Will Include**
- **Data-driven weights** from SHAP analysis
- **Field normalization** functions
- **Error handling** for missing data
- **Score validation** (0-100 range)
- **Metadata generation** for traceability

## Per-Project Regeneration Workflow

### **When to Regenerate**
- âœ… **New project/dataset**: Different data requires different algorithms
- âœ… **Data schema changes**: New fields or field name changes
- âœ… **Microservice retraining**: New ML models = new SHAP importance
- âœ… **Performance issues**: Existing algorithms not performing well
- âœ… **Business context changes**: New market conditions or priorities

### **Regeneration Process**
```bash
# 1. Train new microservice with project data
cd shap-microservice
python automated_model_trainer.py --data new_project_data.csv

# 2. Deploy microservice and generate endpoints with SHAP data
# (Manual deployment step + endpoint generation)

# 3. Regenerate all scoring algorithms from local endpoint data
cd mpiq-ai-chat
python scripts/scoring/generators/regenerate_all_scoring.py \
    --endpoints public/data/endpoints \
    --project new_project_name

# 4. Validate generated algorithms
python scripts/scoring/generators/validator.py \
    --scripts scripts/scoring/*.js

# 5. Execute new scoring to generate enhanced data
bash scripts/scoring/run_all_scoring.sh

# 6. Upload enhanced data to blob storage
python scripts/automation/upload_comprehensive_endpoints.py
```

## Implementation Benefits

### **Data-Driven Accuracy**
- âœ… **Reflects actual data patterns**: Algorithms based on statistical relationships
- âœ… **Objective scoring**: Removes subjective business bias
- âœ… **Adaptive to data changes**: Automatically adjusts to new data characteristics
- âœ… **Scientifically rigorous**: Uses proven ML feature importance

### **Automation and Scalability**
- âœ… **Project-agnostic**: Works with any dataset/industry
- âœ… **Automated regeneration**: One command creates all algorithms
- âœ… **Consistent methodology**: All scripts use same data-driven approach
- âœ… **Validation included**: Built-in testing and quality assurance

### **Maintenance and Evolution**
- âœ… **Self-updating**: Algorithms evolve with data
- âœ… **Traceable**: Clear lineage from SHAP to algorithm
- âœ… **Documented**: Generated algorithms include metadata
- âœ… **Testable**: Comprehensive validation suite

## Technical Architecture

### **File Structure**
```
scripts/scoring/generators/
â”œâ”€â”€ shap_extractor.py           # Extract SHAP from microservice
â”œâ”€â”€ importance_analyzer.py      # Analyze feature importance
â”œâ”€â”€ formula_generator.py        # Create mathematical formulas  
â”œâ”€â”€ js_generator.py            # Generate JavaScript code
â”œâ”€â”€ regenerate_all_scoring.py  # Orchestrate complete process
â”œâ”€â”€ validator.py               # Validate generated scripts
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ scoring_script_template.js
â”‚   â”œâ”€â”€ formula_template.js
â”‚   â””â”€â”€ validation_template.js
â””â”€â”€ config/
    â”œâ”€â”€ analysis_types.json
    â”œâ”€â”€ field_mappings.json
    â””â”€â”€ validation_rules.json

scripts/scoring/
â”œâ”€â”€ strategic-value-scores.js          # Generated scripts
â”œâ”€â”€ competitive-analysis-scores.js     # (Replace existing)
â”œâ”€â”€ demographic-opportunity-scores.js  # (Data-driven)
â””â”€â”€ ... (15+ total)                    # (All regenerated)
```

### **Configuration Management**
```json
// config/analysis_types.json
{
  "strategic": {
    "min_features": 4,
    "max_features": 8,
    "required_field_types": ["market_share", "demographic", "competitive"],
    "score_range": [0, 100],
    "normalization_method": "min_max"
  },
  "competitive": {
    "min_features": 3,
    "max_features": 6,
    "required_field_types": ["brand_metrics", "market_position"],
    "score_range": [0, 100],
    "normalization_method": "z_score"
  }
}
```

## Integration with Existing System

### **Compatibility with Current Workflow**
- âœ… **Same output format**: Generated scripts produce same JSON structure
- âœ… **Same execution method**: `node script.js` still works
- âœ… **Processor compatibility**: No changes needed to analysis processors
- âœ… **Blob storage integration**: Works with existing upload pipeline

### **Enhanced Capabilities**
- âœ… **Better accuracy**: Data-driven algorithms vs. hardcoded weights
- âœ… **Automatic adaptation**: Updates with data changes
- âœ… **Complete coverage**: All 15+ analysis types regenerated
- âœ… **Validation included**: Quality assurance built-in

## Quality Assurance

### **Validation Checks**
1. **Mathematical Validity**: Formulas are mathematically sound
2. **Data Compatibility**: Scripts work with actual data structure
3. **Score Ranges**: All scores fall within 0-100 range  
4. **Field Existence**: All referenced fields exist in data
5. **Performance**: Scripts execute within reasonable time
6. **Consistency**: Similar records get similar scores

### **Testing Strategy**
```bash
# 1. Unit test each generated script
npm test scripts/scoring/strategic-value-scores.js

# 2. Integration test with real data
node scripts/scoring/test_all_generated_scripts.js

# 3. Comparative analysis vs. old algorithms
python scripts/scoring/generators/compare_old_vs_new.py

# 4. Performance benchmarking
python scripts/scoring/generators/benchmark_performance.py
```

## Risk Mitigation

### **Potential Risks**
- **Algorithm instability**: SHAP importance could vary significantly
- **Over-optimization**: Algorithms may overfit to current data
- **Loss of business context**: Pure data-driven may miss strategic priorities
- **Technical complexity**: More complex system to maintain

### **Mitigation Strategies**
- âœ… **Stability analysis**: Track SHAP importance changes over time
- âœ… **Validation suite**: Comprehensive testing before deployment
- âœ… **Fallback capability**: Keep old algorithms as backup
- âœ… **Documentation**: Clear process documentation for maintenance
- âœ… **Monitoring**: Track algorithm performance over time

## Success Metrics

### **Technical Metrics**
- **Score accuracy**: RÂ² correlation with business outcomes
- **Algorithm stability**: Consistency across similar datasets
- **Performance**: Script execution time and resource usage
- **Coverage**: Percentage of data records successfully scored

### **Business Metrics**
- **Analysis quality**: Improved insights from AI analysis
- **User satisfaction**: Better analysis relevance and accuracy
- **Operational efficiency**: Reduced manual algorithm maintenance
- **Adaptability**: Faster time-to-market for new projects

## Implementation Timeline

### **Phase 1: Foundation (Week 1)**
- Build SHAP extraction system
- Create importance analysis framework
- Develop formula generation engine

### **Phase 2: Generation (Week 2)**  
- Build JavaScript code generator
- Create validation and testing suite
- Develop orchestration pipeline

### **Phase 3: Testing (Week 3)**
- Generate algorithms for current project
- Comprehensive testing and validation
- Performance optimization

### **Phase 4: Deployment (Week 4)**
- Deploy new algorithms to production
- Monitor performance and stability
- Documentation and training

## Future Enhancements

### **Advanced Features**
- **Multi-objective optimization**: Balance multiple business goals
- **Dynamic reweighting**: Adjust algorithms based on performance feedback
- **Ensemble methods**: Combine multiple algorithm approaches
- **Real-time adaptation**: Update algorithms as new data arrives

### **Integration Opportunities**
- **A/B testing**: Compare algorithm performance
- **Feedback loops**: Learn from user interactions
- **Business rule overlay**: Allow manual adjustments to data-driven weights
- **Explainable AI**: Enhanced interpretation of algorithm decisions

---

## âœ… Implementation Status

**COMPLETED**: The data-driven scoring algorithm regeneration system has been fully implemented and is ready for production use.

### **What's Been Built**

**Core Components:**
- âœ… **LocalSHAPExtractor** (`scripts/scoring/generators/shap_extractor.py`) - Extracts feature importance from endpoint data
- âœ… **ImportanceAnalyzer** (`scripts/scoring/generators/importance_analyzer.py`) - Analyzes SHAP importance and creates normalized weights
- âœ… **FormulaGenerator** (`scripts/scoring/generators/formula_generator.py`) - Generates mathematical formulas from importance patterns
- âœ… **JavaScriptGenerator** (`scripts/scoring/generators/js_generator.py`) - Creates complete JavaScript scoring scripts
- âœ… **ScoringRegenerator** (`scripts/scoring/generators/regenerate_all_scoring.py`) - Orchestrates complete pipeline
- âœ… **ScoringValidator** (`scripts/scoring/generators/validator.py`) - Validates generated algorithms

**System Features:**
- âœ… **984+ features processed** from existing endpoint data
- âœ… **15+ analysis types supported** (strategic, competitive, demographic, etc.)
- âœ… **Complete validation system** with syntax, mathematical, and data compatibility testing
- âœ… **Project-agnostic design** works with any dataset or target variables
- âœ… **Command-line interface** with comprehensive validation and help
- âœ… **Comprehensive documentation** with usage examples and troubleshooting

### **Usage Commands**

**Quick Start:**
```bash
# Validate system requirements
python scripts/scoring/generators/regenerate_all_scoring.py --validate-only

# Generate all algorithms
python scripts/scoring/generators/regenerate_all_scoring.py --project your_project

# Validate generated scripts
python scripts/scoring/generators/validator.py
```

### **Integration Status**

**âœ… Fully Compatible:**
- Same output format as existing scoring scripts
- Works with existing analysis processors
- Compatible with blob storage upload pipeline
- No client application changes required

**ðŸ“ˆ Enhanced Capabilities:**
- Data-driven weights instead of hardcoded assumptions
- Automatic adaptation to any project's data structure
- Scientific validation and quality assurance
- Complete algorithmic coverage (15+ analysis types)

### **Documentation**

**ðŸ“– Complete Documentation Available:**
- **Implementation Guide**: `docs/DATA_DRIVEN_SCORING_REGENERATION_PLAN.md` (this document)
- **Usage Instructions**: `scripts/automation/SIMPLE_INSTRUCTIONS.md` Step 6.8
- **Field Analysis Strategy**: `docs/RE-SCORING_AND_FIELD_ANALYSIS_PLAN.md`

**ðŸ”§ System Architecture:**
- Modular Python components with clear separation of concerns
- Comprehensive error handling and validation
- Detailed logging and progress reporting
- Extensible design for future enhancements

---

**Key Principle**: This system is designed to be **completely regenerated with every new project** to ensure scoring algorithms always reflect the actual statistical relationships in the current dataset, not outdated assumptions from previous projects.

**Implementation Status**: âœ… **COMPLETE AND READY FOR PRODUCTION**  
**Last Updated**: August 2025  
**System Version**: 1.0  
**Compatibility**: All microservice projects and data formats