#!/usr/bin/env python3
"""
Upload Comprehensive Endpoints to Blob Storage
Upload the 26 comprehensive endpoints generated by the automation pipeline
"""

import os
import sys
from pathlib import Path

# Add the current directory to Python path
sys.path.append(str(Path(__file__).parent))

from blob_uploader import BlobUploader

def main():
    """Upload all 26 comprehensive endpoints and boundary files to blob storage"""
    print("üöÄ Uploading Comprehensive Endpoints and Boundary Files to Blob Storage")
    print("=" * 70)
    
    # Initialize blob uploader for HRB project
    project_root = Path("/Users/voldeck/code/mpiq-ai-chat")
    uploader = BlobUploader(project_root, project_prefix="hrb")
    
    # Check for blob token
    if not uploader.blob_token:
        print("‚ùå BLOB_READ_WRITE_TOKEN environment variable not found")
        print("üí° Set it in your .env.local file:")
        print("   BLOB_READ_WRITE_TOKEN=your_token_here")
        return 1
    
    # Define the 26 comprehensive endpoints we want to upload
    comprehensive_endpoints = [
        # Standard 19 endpoints  
        "strategic-analysis", "competitive-analysis", "demographic-insights",
        "correlation-analysis", "predictive-modeling", "trend-analysis",
        "scenario-analysis", "segment-profiling", "feature-importance-ranking",
        "feature-interactions", "outlier-detection", "spatial-clusters",
        "sensitivity-analysis", "model-performance", "anomaly-detection",
        "comparative-analysis", "brand-difference", "analyze", "customer-profile",
        
        # New 7 comprehensive model endpoints
        "algorithm-comparison", "ensemble-analysis", "model-selection",
        "anomaly-insights", "consensus-analysis", "dimensionality-insights"
    ]
    
    # Check which endpoints exist
    automation_dir = project_root / "scripts" / "automation"
    existing_endpoints = []
    
    for endpoint in comprehensive_endpoints:
        endpoint_file = automation_dir / f"{endpoint}.json"
        if endpoint_file.exists():
            existing_endpoints.append(endpoint)
        else:
            print(f"‚ö†Ô∏è  Endpoint file not found: {endpoint}.json")
    
    if not existing_endpoints:
        print("‚ùå No endpoint files found to upload")
        return 1
    
    print(f"üìÅ Found {len(existing_endpoints)} endpoint files to upload")
    print(f"üì§ Uploading from: {automation_dir}")
    
    # Upload the endpoints
    successful, failed = uploader.upload_from_directory(
        automation_dir, 
        existing_endpoints,
        force_reupload=False  # Only upload new/changed endpoints
    )
    
    # Upload boundary files
    print(f"\nüó∫Ô∏è  UPLOADING BOUNDARY FILES")
    print("-" * 40)
    
    boundary_successful, boundary_failed = uploader.upload_boundary_files(
        force_reupload=False  # Only upload new/changed boundary files
    )
    
    # Print detailed summary
    print("\n" + uploader.generate_upload_summary())
    
    # Final status
    total_successful = successful + boundary_successful
    total_failed = failed + boundary_failed
    
    if total_failed == 0:
        print("üéâ All endpoints and boundary files successfully uploaded to blob storage!")
        print("‚úÖ Client applications will now automatically use blob URLs for large files")
        if boundary_successful > 0:
            print("üó∫Ô∏è  Geographic visualizations will now load boundary data from blob storage")
    else:
        print(f"‚ö†Ô∏è  {total_failed} files failed to upload, but {total_successful} succeeded")
        print("üí° Failed files are still available locally")
        if boundary_failed > 0:
            print("‚ö†Ô∏è  Some boundary files failed to upload - geographic visualizations may be affected")
    
    return 0 if total_failed == 0 else 1

if __name__ == "__main__":
    exit(main())