#!/usr/bin/env python3
"""
Master Automation Script - Complete ArcGIS to Microservice Pipeline
Part of the ArcGIS to Microservice Automation Pipeline

Orchestrates all automation components in a single end-to-end pipeline:
1. Service Discovery & Analysis
2. Data Extraction
3. Field Mapping
4. Model Training
5. Endpoint Generation
6. Score Calculation
7. Layer Configuration Generation

Usage:
    python run_complete_automation.py "https://services8.arcgis.com/.../FeatureServer" --project nike_2025
"""

import asyncio
import argparse
import json
import logging
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

# Import automation components
from arcgis_service_inspector import ArcGISServiceInspector
from arcgis_data_extractor import ArcGISDataExtractor
from intelligent_field_mapper import IntelligentFieldMapper
from automated_model_trainer import AutomatedModelTrainer
from endpoint_generator import EndpointGenerator
from comprehensive_endpoint_generator import ComprehensiveEndpointGenerator
from automated_score_calculator import AutomatedScoreCalculator
from layer_config_generator import LayerConfigGenerator
from blob_uploader import BlobUploader

class CompleteAutomationPipeline:
    """
    Master automation pipeline that orchestrates all components
    """
    
    def __init__(self, service_url: str, project_name: str, config: Optional[Dict] = None):
        self.service_url = service_url
        self.project_name = project_name
        self.config = config or {}
        self.start_time = time.time()
        self.target_variable = config.get('target_variable', 'MP10128A_B_P')  # Default to H&R Block Online usage
        
        # Setup project directories
        self.project_root = Path("/Users/voldeck/code/mpiq-ai-chat")
        self.output_dir = self.project_root / "projects" / project_name
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # Setup comprehensive logging
        self._setup_logging()
        
        # Pipeline state tracking
        self.pipeline_state = {
            'status': 'initialized',
            'current_phase': None,
            'phases_completed': [],
            'phases_failed': [],
            'start_time': datetime.now().isoformat(),
            'metadata': {}
        }
        
        # Component results storage
        self.results = {
            'service_analysis': None,
            'extracted_data': None,
            'field_mappings': None,
            'model_training': None,
            'endpoints': None,
            'scores': None,
            'layer_configs': None
        }
        
    def _setup_logging(self):
        """Setup comprehensive logging for the pipeline"""
        log_file = self.output_dir / f"automation_pipeline_{datetime.now():%Y%m%d_%H%M%S}.log"
        
        # Create formatters
        detailed_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        simple_formatter = logging.Formatter('%(message)s')
        
        # Setup root logger
        self.logger = logging.getLogger('AutomationPipeline')
        self.logger.setLevel(logging.INFO)
        
        # File handler (detailed)
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(detailed_formatter)
        self.logger.addHandler(file_handler)
        
        # Console handler (simple)
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(simple_formatter)
        self.logger.addHandler(console_handler)
        
        # Silence other loggers to avoid noise
        logging.getLogger('requests').setLevel(logging.WARNING)
        logging.getLogger('urllib3').setLevel(logging.WARNING)
    
    async def run_complete_pipeline(self) -> bool:
        """
        Execute the complete automation pipeline
        """
        try:
            self.logger.info("üöÄ Starting Complete ArcGIS to Microservice Automation Pipeline")
            self.logger.info(f"üìÖ Project: {self.project_name}")
            self.logger.info(f"üåê Service: {self.service_url}")
            self.logger.info(f"üéØ Target Variable: {self.target_variable} (Used H&R Block Online to Prepare Taxes)")
            self.logger.info("=" * 80)
            self.logger.info("")
            self.logger.info("‚ö†Ô∏è  WARNING: This pipeline will modify your existing files:")
            self.logger.info("   ‚Ä¢ config/layers.ts will be REPLACED (original backed up)")
            self.logger.info("   ‚Ä¢ 18 new endpoint files will be created in public/data/endpoints/")
            self.logger.info("   ‚Ä¢ Project files will be created in projects/" + self.project_name)
            self.logger.info("")
            self.logger.info("üîí Safety: All original files are automatically backed up")
            self.logger.info("=" * 80)
            
            # Phase 1: Service Discovery & Analysis
            success = await self._phase_1_service_discovery()
            if not success:
                return False
            
            # Phase 2: Data Extraction
            success = await self._phase_2_data_extraction()
            if not success:
                return False
            
            # Phase 3: Field Mapping
            success = await self._phase_3_field_mapping()
            if not success:
                return False
            
            # Phase 4: Model Training
            success = await self._phase_4_model_training()
            if not success:
                return False
            
            # Phase 5: Endpoint Generation
            success = await self._phase_5_endpoint_generation()
            if not success:
                return False
            
            # Phase 6: Score Calculation
            success = await self._phase_6_score_calculation()
            if not success:
                return False
            
            # Phase 6.5: Field Mapping Update (NEW)
            success = await self._phase_6_5_field_mapping_update()
            if not success:
                return False
            
            # Phase 6.6: Boundary File Verification (NEW)
            success = await self._phase_6_6_boundary_file_verification()
            if not success:
                return False
            
            # Phase 7: Layer Configuration Generation
            success = await self._phase_7_layer_configuration()
            if not success:
                return False
            
            # Phase 7.5: Enhanced Layer Categorization (NEW)
            success = await self._phase_7_5_enhanced_layer_categorization()
            if not success:
                return False
            
            # Phase 8: Final Integration & Deployment
            success = await self._phase_8_final_integration()
            if not success:
                return False
            
            # Generate comprehensive report
            await self._generate_final_report()
            
            self.logger.info("üéâ AUTOMATION PIPELINE COMPLETED SUCCESSFULLY!")
            elapsed_time = (time.time() - self.start_time) / 60
            self.logger.info(f"‚è±Ô∏è  Total execution time: {elapsed_time:.1f} minutes")
            
            # Offer cleanup option
            await self._offer_cleanup()
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Pipeline failed with error: {str(e)}", exc_info=True)
            self.pipeline_state['status'] = 'failed'
            self.pipeline_state['error'] = str(e)
            return False
    
    async def _phase_1_service_discovery(self) -> bool:
        """Phase 1: Service Discovery & Analysis"""
        self.logger.info("üîç PHASE 1: Service Discovery & Analysis")
        self.logger.info("-" * 50)
        
        self.pipeline_state['current_phase'] = 'service_discovery'
        
        try:
            # Initialize service inspector
            inspector = ArcGISServiceInspector(self.service_url)
            
            # Discover layers
            self.logger.info("üìä Discovering service layers...")
            layers = inspector.discover_layers()
            
            if not layers:
                self.logger.error("‚ùå No layers found in service")
                return False
            
            # Generate extraction configuration
            self.logger.info("‚öôÔ∏è  Generating extraction configuration...")
            extraction_config = inspector.generate_extraction_config()
            
            # Save results
            config_file = self.output_dir / "service_analysis.json"
            with open(config_file, 'w') as f:
                json.dump({
                    'layers': [layer.__dict__ if hasattr(layer, '__dict__') else layer for layer in layers],
                    'extraction_config': extraction_config,
                    'analysis_timestamp': datetime.now().isoformat()
                }, f, indent=2, default=str)
            
            # Store results
            self.results['service_analysis'] = {
                'layers': layers,
                'config': extraction_config,
                'layer_count': len(layers)
            }
            
            self.logger.info(f"‚úÖ Phase 1 Complete: Discovered {len(layers)} layers")
            self.pipeline_state['phases_completed'].append('service_discovery')
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase 1 Failed: {str(e)}")
            self.pipeline_state['phases_failed'].append('service_discovery')
            return False
    
    async def _phase_2_data_extraction(self) -> bool:
        """Phase 2: Data Extraction"""
        self.logger.info("\nüìä PHASE 2: Data Extraction")
        self.logger.info("-" * 50)
        
        self.pipeline_state['current_phase'] = 'data_extraction'
        
        try:
            extraction_config = self.results['service_analysis']['config']
            
            # Initialize data extractor with service URL and output directory
            extractor = ArcGISDataExtractor(self.service_url, str(self.output_dir))
            
            # Extract all layers
            self.logger.info("‚¨áÔ∏è  Extracting data from all layers...")
            extraction_summary = await extractor.extract_all_data()
            
            if not extraction_summary or not extraction_summary.get('success'):
                self.logger.error("‚ùå No data extracted from service")
                return False
            
            # Merge datasets from extracted CSV files
            self.logger.info("üîÑ Merging datasets...")
            import pandas as pd
            from pathlib import Path
            
            # Find all extracted CSV files
            csv_files = list(self.output_dir.glob("layer_*.csv"))
            if not csv_files:
                self.logger.error("‚ùå No CSV files found to merge")
                return False
            
            # Read and merge all CSV files
            dfs = []
            for csv_file in csv_files:
                try:
                    df = pd.read_csv(csv_file)
                    if not df.empty:
                        dfs.append(df)
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Failed to read {csv_file.name}: {str(e)}")
            
            if not dfs:
                self.logger.error("‚ùå No valid data found in CSV files")
                return False
            
            # Merge on common geographic identifiers
            merged_data = dfs[0]
            for df in dfs[1:]:
                # Try to merge on common ID fields
                common_cols = set(merged_data.columns) & set(df.columns)
                id_cols = [col for col in common_cols if 'ID' in col.upper() or 'OBJECTID' in col]
                if id_cols:
                    merged_data = pd.merge(merged_data, df, on=id_cols[0], how='outer', suffixes=('', '_dup'))
                    # Remove duplicate columns
                    merged_data = merged_data.loc[:, ~merged_data.columns.str.endswith('_dup')]
                else:
                    # If no common ID, concatenate
                    merged_data = pd.concat([merged_data, df], axis=1)
            
            # Save merged dataset
            merged_path = self.output_dir / "merged_dataset.csv"
            merged_data.to_csv(merged_path, index=False)
            
            # Store results
            self.results['extracted_data'] = {
                'merged_data': merged_data,
                'merged_path': merged_path,
                'record_count': len(merged_data),
                'field_count': len(merged_data.columns)
            }
            
            self.logger.info(f"‚úÖ Phase 2 Complete: Extracted {len(merged_data):,} records with {len(merged_data.columns)} fields")
            self.pipeline_state['phases_completed'].append('data_extraction')
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase 2 Failed: {str(e)}")
            self.pipeline_state['phases_failed'].append('data_extraction')
            return False
    
    async def _phase_3_field_mapping(self) -> bool:
        """Phase 3: Field Mapping"""
        self.logger.info("\nü§ñ PHASE 3: Intelligent Field Mapping")
        self.logger.info("-" * 50)
        
        self.pipeline_state['current_phase'] = 'field_mapping'
        
        try:
            merged_data = self.results['extracted_data']['merged_data']
            
            # Initialize field mapper
            mapper = IntelligentFieldMapper()
            
            # Generate automatic mappings
            self.logger.info("üéØ Generating intelligent field mappings...")
            # Convert DataFrame to field data format for the mapper
            field_data = [{'name': col, 'type': str(merged_data[col].dtype)} for col in merged_data.columns]
            field_mappings = mapper.generate_intelligent_mappings(field_data)
            
            # Create simple validation summary
            self.logger.info("‚úîÔ∏è  Creating validation summary...")
            validation = {
                'total_fields': len(field_data),
                'mapped_fields': len(field_mappings.get('mappings', {})),
                'unmapped_fields': len(field_mappings.get('unmapped', [])),
                'validation_passed': True
            }
            
            # Save mappings
            mappings_file = self.output_dir / "field_mappings.json"
            with open(mappings_file, 'w') as f:
                json.dump({
                    'mappings': field_mappings,
                    'validation': validation,
                    'mapping_timestamp': datetime.now().isoformat()
                }, f, indent=2, default=str)
            
            # Store results
            self.results['field_mappings'] = {
                'mappings': field_mappings,
                'validation': validation,
                'mapped_field_count': len(field_mappings)
            }
            
            self.logger.info(f"‚úÖ Phase 3 Complete: Mapped {len(field_mappings)} fields")
            if validation.get('missing_required'):
                self.logger.warning(f"‚ö†Ô∏è  Missing required fields: {validation['missing_required']}")
            
            self.pipeline_state['phases_completed'].append('field_mapping')
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase 3 Failed: {str(e)}")
            self.pipeline_state['phases_failed'].append('field_mapping')
            return False
    
    async def _phase_4_model_training(self) -> bool:
        """Phase 4: Model Training & Microservice Package Creation"""
        self.logger.info("\nüéì PHASE 4: Model Training & Microservice Package Creation")
        self.logger.info("-" * 50)
        
        self.pipeline_state['current_phase'] = 'model_training'
        
        try:
            merged_path = self.results['extracted_data']['merged_path']
            field_mappings = self.results['field_mappings']['mappings']
            
            # Initialize model trainer with output directory, not CSV path
            model_output_dir = self.output_dir / "trained_models"
            trainer = AutomatedModelTrainer(str(model_output_dir))
            
            # Train comprehensive models with specified target variable
            self.logger.info("üß† Training comprehensive XGBoost models...")
            self.logger.info(f"üéØ Using target variable: {self.target_variable}")
            training_results = trainer.train_comprehensive_models(str(merged_path), target_variable=self.target_variable)
            
            if not training_results:
                self.logger.error("‚ùå Model training failed - no results returned")
                return False
            
            # Check if any models were successfully trained
            successful_models = [k for k, v in training_results.items() 
                                if isinstance(v, dict) and v.get('success') is not False]
            
            if not successful_models:
                self.logger.error("‚ùå Model training failed - no successful models")
                return False
            
            # Generate comprehensive performance report
            self.logger.info("üìä Generating model performance report...")
            try:
                from model_performance_reporter import ModelPerformanceReporter
                
                reporter = ModelPerformanceReporter(self.project_name, self.output_dir)
                performance_report = reporter.generate_performance_report(training_results, self.target_variable)
                
                # Store performance report in results
                self.results['performance_report'] = performance_report
                
                self.logger.info("‚úÖ Model performance report generated successfully")
                self.logger.info(f"üìà Dashboard saved: {self.output_dir}/MODEL_PERFORMANCE_DASHBOARD.html")
                self.logger.info(f"üìä Report saved: {self.output_dir}/MODEL_PERFORMANCE_REPORT.json")
                
            except Exception as perf_error:
                self.logger.warning(f"‚ö†Ô∏è  Performance report generation failed: {perf_error}")
                self.logger.info("üìä Continuing with pipeline despite performance report failure...")
                self.results['performance_report'] = None
            
            # Extract validation results
            validation_results = {
                'passed': True,
                'models_trained': len(successful_models),
                'successful_models': successful_models,
                'metrics': training_results
            }
            
            # Create microservice deployment package
            self.logger.info("üì¶ Creating microservice deployment package...")
            microservice_package = await self._create_microservice_package(merged_path, field_mappings, validation_results)
            
            # Store results
            self.results['model_training'] = {
                'validation': validation_results,
                'training_successful': validation_results.get('passed', False),
                'microservice_package': microservice_package
            }
            
            if validation_results.get('passed', False):
                self.logger.info("‚úÖ Phase 4 Complete: Model training successful")
                self.logger.info(f"üì¶ Microservice package created: {microservice_package['package_path']}")
            else:
                self.logger.warning("‚ö†Ô∏è  Phase 4 Warning: Model validation had issues but continuing...")
            
            self.pipeline_state['phases_completed'].append('model_training')
            
            # Pause for manual microservice deployment
            await self._pause_for_microservice_deployment(microservice_package)
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase 4 Failed: {str(e)}")
            self.pipeline_state['phases_failed'].append('model_training')
            return False
    
    async def _phase_5_endpoint_generation(self) -> bool:
        """Phase 5: Endpoint Generation (Standard + Comprehensive)"""
        self.logger.info("\nüìä PHASE 5: Endpoint Generation (26 Total)")
        self.logger.info("-" * 50)
        
        self.pipeline_state['current_phase'] = 'endpoint_generation'
        
        try:
            merged_path = self.results['extracted_data']['merged_path']
            models_dir = self.results.get('trained_models', {}).get('models_dir')
            
            # Try to use ComprehensiveEndpointGenerator first for all 26 endpoints
            try:
                # Initialize comprehensive endpoint generator (19 standard + 7 new)
                self.logger.info("üèóÔ∏è  Generating 26 comprehensive endpoint configurations...")
                self.logger.info("   üìä 19 Standard endpoints")
                self.logger.info("   üß† 7 Comprehensive model endpoints")
                
                if models_dir and Path(models_dir).exists():
                    generator = ComprehensiveEndpointGenerator(models_dir=str(models_dir))
                else:
                    # Fallback with default models directory path
                    default_models_dir = self.project_dir / "trained_models"
                    generator = ComprehensiveEndpointGenerator(models_dir=str(default_models_dir))
                
                # Generate all 26 endpoints
                endpoints = generator.generate_all_comprehensive_endpoints(str(merged_path))
                
                # Count successful endpoints
                successful_endpoints = [e for e in endpoints.values() if e.get('success', True)]
                
                self.logger.info(f"   ‚úÖ Successfully generated {len(successful_endpoints)}/26 endpoints")
                
            except Exception as e:
                # Fallback to standard generator if comprehensive fails
                self.logger.warning(f"‚ö†Ô∏è Comprehensive generator failed: {str(e)}")
                self.logger.info("üìä Falling back to standard endpoint generation...")
                
                generator = EndpointGenerator(str(merged_path))
                endpoints = generator.generate_all_endpoints(str(merged_path))
            
            if not endpoints:
                self.logger.error("‚ùå No endpoints generated")
                return False
            
            # Store results
            self.results['endpoints'] = {
                'endpoints': endpoints,
                'endpoint_count': len(endpoints),
                'standard_count': 19,
                'comprehensive_count': 7
            }
            
            self.logger.info(f"‚úÖ Phase 5 Complete: Generated {len(endpoints)} endpoints")
            
            # Generate model traceability report
            self.logger.info("üîç Generating model traceability report...")
            try:
                from model_traceability_viewer import ModelTraceabilityViewer
                
                viewer = ModelTraceabilityViewer(str(self.output_dir / "generated_endpoints"))
                traceability_report = viewer.generate_model_traceability_report(
                    str(self.output_dir / "MODEL_TRACEABILITY_REPORT.html")
                )
                
                self.logger.info("‚úÖ Model traceability report generated")
                self.logger.info(f"üìã Report saved: {self.output_dir}/MODEL_TRACEABILITY_REPORT.html")
                
                # Store traceability summary in results
                self.results['model_traceability'] = viewer.get_all_endpoint_models()
                
            except Exception as trace_error:
                self.logger.warning(f"‚ö†Ô∏è  Traceability report generation failed: {trace_error}")
                self.logger.info("üìä Continuing with pipeline despite traceability report failure...")
                self.results['model_traceability'] = None
            
            self.pipeline_state['phases_completed'].append('endpoint_generation')
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase 5 Failed: {str(e)}")
            self.pipeline_state['phases_failed'].append('endpoint_generation')
            return False
    
    async def _phase_6_score_calculation(self) -> bool:
        """Phase 6: Score Calculation"""
        self.logger.info("\nüìà PHASE 6: Score Calculation")
        self.logger.info("-" * 50)
        
        self.pipeline_state['current_phase'] = 'score_calculation'
        
        try:
            endpoints = self.results['endpoints']['endpoints']
            
            # Initialize score calculator
            calculator = AutomatedScoreCalculator()
            
            # Apply all scoring algorithms
            self.logger.info("üßÆ Applying comprehensive scoring algorithms...")
            scoring_results = calculator.apply_all_scoring_algorithms()
            
            if not scoring_results:
                self.logger.error("‚ùå Scoring failed")
                return False
            
            scored_endpoints = scoring_results.get('endpoints_processed', 0)
            
            # Store updated results
            self.results['endpoints']['endpoints'] = endpoints
            self.results['scores'] = {
                'scored_endpoints': scored_endpoints,
                'total_endpoints': len(endpoints)
            }
            
            self.logger.info(f"‚úÖ Phase 6 Complete: Applied scores to {scored_endpoints} endpoints")
            self.pipeline_state['phases_completed'].append('score_calculation')
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase 6 Failed: {str(e)}")
            self.pipeline_state['phases_failed'].append('score_calculation')
            return False
    
    async def _phase_6_5_field_mapping_update(self) -> bool:
        """Phase 6.5: Update Field Mappings from Generated Endpoints"""
        self.logger.info("\nüóÇÔ∏è  PHASE 6.5: Field Mapping Update")
        self.logger.info("-" * 50)
        
        self.pipeline_state['current_phase'] = 'field_mapping_update'
        
        try:
            # Path to the Node.js scripts
            project_root = Path("/Users/voldeck/code/mpiq-ai-chat")
            extract_script = project_root / "scripts" / "extract-all-fields.js"
            generate_script = project_root / "scripts" / "generate-field-mappings.js"
            
            self.logger.info("üîç Extracting all unique fields from endpoint data...")
            
            # Run field extraction script
            extract_result = subprocess.run([
                "node", str(extract_script)
            ], cwd=str(project_root), capture_output=True, text=True)
            
            if extract_result.returncode != 0:
                self.logger.error(f"‚ùå Field extraction failed: {extract_result.stderr}")
                return False
            
            self.logger.info("üìä Field extraction completed successfully")
            self.logger.info(extract_result.stdout.strip())
            
            # Run field mapping generator
            self.logger.info("\nüîß Generating updated field mapping files...")
            generate_result = subprocess.run([
                "node", str(generate_script)
            ], cwd=str(project_root), capture_output=True, text=True)
            
            if generate_result.returncode != 0:
                self.logger.error(f"‚ùå Field mapping generation failed: {generate_result.stderr}")
                return False
            
            self.logger.info("‚úÖ Field mapping files updated successfully")
            self.logger.info(generate_result.stdout.strip())
            
            # Store results
            self.results['field_mappings_update'] = {
                'extraction_output': extract_result.stdout,
                'generation_output': generate_result.stdout,
                'updated_timestamp': datetime.now().isoformat()
            }
            
            # Alert about synonym expansion
            self.logger.info("\n‚ö†Ô∏è  FIELD MAPPING ENHANCEMENT NEEDED:")
            self.logger.info("üîç Consider expanding field aliases with robust synonym lists")
            self.logger.info("üìù See: docs/FIELD_MAPPING_AUTOMATION.md for guidance")
            self.logger.info("üéØ This will improve natural language query processing")
            
            self.logger.info("‚úÖ Phase 6.5 Complete: Field mappings synchronized with current data")
            self.pipeline_state['phases_completed'].append('field_mapping_update')
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase 6.5 Failed: {str(e)}")
            self.pipeline_state['phases_failed'].append('field_mapping_update')
            return False
    
    async def _phase_6_6_boundary_file_verification(self) -> bool:
        """Phase 6.6: Boundary File Verification and Generation"""
        self.logger.info("\nüó∫Ô∏è  PHASE 6.6: Boundary File Verification")
        self.logger.info("-" * 50)
        
        self.pipeline_state['current_phase'] = 'boundary_verification'
        
        try:
            # Check for existing boundary files
            boundaries_dir = self.project_root / "public" / "data" / "boundaries"
            zip_boundaries = boundaries_dir / "zip_boundaries.json"
            fsa_boundaries = boundaries_dir / "fsa_boundaries.json"
            
            boundary_files_exist = False
            existing_files = []
            
            # Check for ZIP code boundaries
            if zip_boundaries.exists():
                file_size = zip_boundaries.stat().st_size
                self.logger.info(f"‚úÖ Found ZIP boundaries file: {zip_boundaries.name} ({file_size / 1024 / 1024:.1f}MB)")
                existing_files.append(f"ZIP codes ({file_size / 1024 / 1024:.1f}MB)")
                boundary_files_exist = True
            
            # Check for FSA boundaries (Canadian postal codes)
            if fsa_boundaries.exists():
                file_size = fsa_boundaries.stat().st_size
                self.logger.info(f"‚úÖ Found FSA boundaries file: {fsa_boundaries.name} ({file_size / 1024 / 1024:.1f}MB)")
                existing_files.append(f"FSA codes ({file_size / 1024 / 1024:.1f}MB)")
                boundary_files_exist = True
            
            # Check for other boundary files
            if boundaries_dir.exists():
                other_boundaries = list(boundaries_dir.glob("*boundaries*.json"))
                for boundary_file in other_boundaries:
                    if boundary_file.name not in ["zip_boundaries.json", "fsa_boundaries.json"]:
                        file_size = boundary_file.stat().st_size
                        self.logger.info(f"‚úÖ Found boundary file: {boundary_file.name} ({file_size / 1024 / 1024:.1f}MB)")
                        existing_files.append(f"{boundary_file.stem} ({file_size / 1024 / 1024:.1f}MB)")
                        boundary_files_exist = True
            
            if boundary_files_exist:
                self.logger.info(f"üéØ Boundary files verified: {', '.join(existing_files)}")
                
                # Check if files should be uploaded to blob storage
                self.logger.info("‚òÅÔ∏è  Checking blob storage for boundary files...")
                
                # Check if blob token is available for upload
                blob_token = os.getenv('BLOB_READ_WRITE_TOKEN')
                if blob_token:
                    from .blob_uploader import BlobUploader
                    uploader = BlobUploader(self.project_root, "hrb")
                    
                    # Check existing blob URLs
                    blob_urls = uploader.load_existing_blob_urls()
                    boundary_keys = [key for key in blob_urls.keys() if key.startswith('boundaries/')]
                    
                    if boundary_keys:
                        self.logger.info(f"‚úÖ Found {len(boundary_keys)} boundary files in blob storage")
                        for key in boundary_keys:
                            self.logger.info(f"   ‚Ä¢ {key}")
                    else:
                        self.logger.info("‚ö†Ô∏è  No boundary files found in blob storage")
                        self.logger.info("üí° RECOMMENDATION: Upload boundary files to blob storage")
                        self.logger.info("   Run: python scripts/automation/upload_comprehensive_endpoints.py")
                        self.logger.info("   This will upload both endpoints and boundary files to improve performance")
                else:
                    self.logger.info("üí° BLOB STORAGE: Set BLOB_READ_WRITE_TOKEN to enable boundary file cloud storage")
                
                # Store results
                self.results['boundary_verification'] = {
                    'status': 'verified',
                    'existing_files': existing_files,
                    'boundaries_directory': str(boundaries_dir),
                    'blob_storage_available': bool(blob_token),
                    'blob_boundary_count': len([key for key in (uploader.load_existing_blob_urls().keys() if blob_token else []) if key.startswith('boundaries/')])
                }
                
                self.logger.info("‚úÖ Phase 6.6 Complete: Geographic boundary files are available")
                self.pipeline_state['phases_completed'].append('boundary_verification')
                
                return True
            else:
                # Alert about missing boundary files
                self.logger.warning("‚ö†Ô∏è  NO BOUNDARY FILES FOUND!")
                self.logger.warning("üó∫Ô∏è  Geographic boundary files are required for spatial analysis")
                self.logger.warning(f"üìÅ Expected location: {boundaries_dir}")
                self.logger.warning("")
                self.logger.warning("üö® REQUIRED ACTION:")
                self.logger.warning("1. Obtain boundary files for your geographic region:")
                self.logger.warning("   ‚Ä¢ ZIP codes for US data: zip_boundaries.json")  
                self.logger.warning("   ‚Ä¢ FSA codes for Canadian data: fsa_boundaries.json")
                self.logger.warning("   ‚Ä¢ Other regional boundaries as needed")
                self.logger.warning("2. Place boundary files in: public/data/boundaries/")
                self.logger.warning("3. Ensure files are in GeoJSON format")
                self.logger.warning("4. Re-run the automation pipeline")
                self.logger.warning("")
                self.logger.warning("üìñ Common boundary sources:")
                self.logger.warning("   ‚Ä¢ US Census Bureau (ZIP codes)")
                self.logger.warning("   ‚Ä¢ Statistics Canada (FSA codes)")  
                self.logger.warning("   ‚Ä¢ Natural Earth (countries/states)")
                self.logger.warning("   ‚Ä¢ OpenStreetMap (various boundaries)")
                
                # Store alert results
                self.results['boundary_verification'] = {
                    'status': 'missing',
                    'alert_issued': True,
                    'required_location': str(boundaries_dir),
                    'required_files': ['zip_boundaries.json', 'fsa_boundaries.json']
                }
                
                self.logger.info("‚ö†Ô∏è  Phase 6.6 Complete: Boundary file alert issued")
                self.pipeline_state['phases_completed'].append('boundary_verification')
                
                return True  # Continue pipeline but with alert
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase 6.6 Failed: {str(e)}")
            self.pipeline_state['phases_failed'].append('boundary_verification')
            return False
    
    async def _phase_7_layer_configuration(self) -> bool:
        """Phase 7: Layer Configuration Generation"""
        self.logger.info("\nüèóÔ∏è  PHASE 7: Layer Configuration Generation")
        self.logger.info("-" * 50)
        
        self.pipeline_state['current_phase'] = 'layer_configuration'
        
        try:
            # Initialize layer config generator
            layer_generator = LayerConfigGenerator(str(self.project_root))
            
            # Analyze service for layer configuration
            self.logger.info("üîç Analyzing service for layer configurations...")
            layers = layer_generator.analyze_arcgis_service(self.service_url)
            
            if not layers:
                self.logger.error("‚ùå No layers found for configuration generation")
                return False
            
            # Generate and save layer configuration
            self.logger.info("‚öôÔ∏è  Generating TypeScript layer configurations...")
            output_filename = f"layers_{self.project_name}.ts"
            success = layer_generator.save_layer_configuration(layers, output_filename)
            
            if not success:
                self.logger.error("‚ùå Failed to save layer configuration")
                return False
            
            # Store results
            self.results['layer_configs'] = {
                'layers': layers,
                'layer_count': len(layers),
                'output_file': output_filename
            }
            
            self.logger.info(f"‚úÖ Phase 7 Complete: Generated configuration for {len(layers)} layers")
            self.pipeline_state['phases_completed'].append('layer_configuration')
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase 7 Failed: {str(e)}")
            self.pipeline_state['phases_failed'].append('layer_configuration')
            return False
    
    async def _phase_7_5_enhanced_layer_categorization(self) -> bool:
        """Phase 7.5: Enhanced Layer Categorization Post-Processing"""
        self.logger.info("\nüè∑Ô∏è  PHASE 7.5: Enhanced Layer Categorization")
        self.logger.info("-" * 50)
        
        self.pipeline_state['current_phase'] = 'enhanced_layer_categorization'
        
        try:
            from layer_categorization_post_processor import LayerCategorizationPostProcessor
            
            self.logger.info("üîÑ Applying enhanced semantic categorization to layers.ts...")
            self.logger.info("This phase runs AFTER layers.ts generation with operational enhancements:")
            self.logger.info("  ‚Ä¢ Automatic point layer detection ‚Üí 'Locations' category")
            self.logger.info("  ‚Ä¢ Custom category support")
            self.logger.info("  ‚Ä¢ Layer exclusion patterns")
            self.logger.info("  ‚Ä¢ Fallback strategies for uncategorized layers")
            self.logger.info("  ‚Ä¢ Manual correction system")
            
            # Initialize post-processor
            processor = LayerCategorizationPostProcessor(str(self.project_root))
            
            # Run enhanced categorization
            success = processor.run_complete_post_processing()
            
            if not success:
                self.logger.error("‚ùå Enhanced categorization failed")
                return False
            
            # Store results for reporting
            self.results['enhanced_categorization'] = {
                'total_layers': len(processor.categorization_results),
                'excluded_layers': len(processor.excluded_layers),
                'point_layers_found': len(processor.point_layers_found),
                'corrections_applied': len(processor.corrections_applied),
                'uncategorized_layers': len(processor.uncategorized_layers),
                'categorization_results': processor.categorization_results
            }
            
            # Log key metrics
            results = self.results['enhanced_categorization']
            self.logger.info("üìä Enhanced Categorization Results:")
            self.logger.info(f"  ‚úÖ Total layers processed: {results['total_layers']}")
            self.logger.info(f"  üìç Point layers auto-assigned: {results['point_layers_found']}")
            self.logger.info(f"  üö´ Layers excluded: {results['excluded_layers']}")
            self.logger.info(f"  üîß Manual corrections: {results['corrections_applied']}")
            self.logger.info(f"  ‚ùì Uncategorized (fallback applied): {results['uncategorized_layers']}")
            
            if results['point_layers_found'] > 0:
                self.logger.info(f"üéØ Point layers automatically assigned to 'Locations' category")
            
            if results['uncategorized_layers'] > 0:
                self.logger.info(f"‚ö†Ô∏è  {results['uncategorized_layers']} layers used fallback categorization")
                self.logger.info("üí° Consider adding custom categories or corrections for better accuracy")
            
            self.logger.info("‚úÖ Phase 7.5 Complete: Enhanced semantic categorization applied")
            self.pipeline_state['phases_completed'].append('enhanced_layer_categorization')
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase 7.5 Failed: {str(e)}")
            self.pipeline_state['phases_failed'].append('enhanced_layer_categorization')
            return False
    
    async def _phase_8_final_integration(self) -> bool:
        """Phase 8: Final Integration & Deployment Preparation"""
        self.logger.info("\nüöÄ PHASE 8: Final Integration & Deployment")
        self.logger.info("-" * 50)
        
        self.pipeline_state['current_phase'] = 'final_integration'
        
        try:
            # Copy endpoints to public data directory
            self.logger.info("üìÅ Copying endpoints to public data directory...")
            endpoints_dir = self.project_root / "public" / "data" / "endpoints"
            endpoints_dir.mkdir(exist_ok=True, parents=True)
            
            endpoints = self.results['endpoints']['endpoints']
            copied_files = 0
            
            for endpoint_name, endpoint_data in endpoints.items():
                endpoint_file = endpoints_dir / f"{endpoint_name}.json"
                with open(endpoint_file, 'w') as f:
                    json.dump(endpoint_data, f, indent=2, default=str)
                copied_files += 1
            
            # Upload endpoints to Vercel Blob storage
            self.logger.info("‚òÅÔ∏è  Uploading endpoints to Vercel Blob storage...")
            blob_uploader = BlobUploader(self.project_root)
            
            # Check if blob token is available
            if blob_uploader.blob_token:
                self.logger.info(f"üì§ Uploading {len(endpoints)} endpoints to blob storage...")
                successful_uploads, failed_uploads = blob_uploader.upload_endpoints(endpoints, force_reupload=False)
                
                if successful_uploads > 0:
                    self.logger.info(f"‚úÖ Successfully uploaded {successful_uploads} endpoints to blob storage")
                    
                if failed_uploads > 0:
                    self.logger.warning(f"‚ö†Ô∏è  {failed_uploads} endpoints failed to upload to blob storage")
                    self.logger.info("üí° Endpoints are still available locally in public/data/endpoints/")
                
                # Upload boundary files to blob storage
                self.logger.info("üó∫Ô∏è  Uploading boundary files to blob storage...")
                boundary_successful, boundary_failed = blob_uploader.upload_boundary_files(force_reupload=False)
                
                if boundary_successful > 0:
                    self.logger.info(f"‚úÖ Successfully uploaded {boundary_successful} boundary files to blob storage")
                elif boundary_failed > 0:
                    self.logger.warning(f"‚ö†Ô∏è  {boundary_failed} boundary files failed to upload to blob storage")
                else:
                    self.logger.info("üìç No boundary files found to upload")
                
                # Log combined upload summary
                total_successful = successful_uploads + boundary_successful
                total_failed = failed_uploads + boundary_failed
                
                if total_successful > 0 or total_failed > 0:
                    self.logger.info("\n" + blob_uploader.generate_upload_summary())
                    
                    if boundary_successful > 0:
                        self.logger.info("üó∫Ô∏è  Geographic visualizations will now load boundary data from blob storage")
                        
            else:
                self.logger.warning("‚ö†Ô∏è  BLOB_READ_WRITE_TOKEN not found - skipping blob upload")
                self.logger.info("üí° Set BLOB_READ_WRITE_TOKEN in .env.local to enable blob storage upload")
                self.logger.info("üìÅ Endpoints are available locally in public/data/endpoints/")
                self.logger.info("üó∫Ô∏è  Boundary files are available locally in public/data/boundaries/")
            
            # Update layer configuration in main config
            self.logger.info("‚öôÔ∏è  Updating main layer configuration...")
            layer_config_file = self.project_root / "config" / "layers.ts"
            generated_config_file = self.project_root / "config" / f"layers_{self.project_name}.ts"
            
            if generated_config_file.exists():
                # Backup existing and replace
                if layer_config_file.exists():
                    backup_file = layer_config_file.with_suffix('.ts.backup')
                    self.logger.info(f"üì¶ Creating backup: {layer_config_file} ‚Üí {backup_file.name}")
                    layer_config_file.rename(backup_file)
                    self.logger.info("‚úÖ Original file safely backed up")
                
                self.logger.info(f"üîÑ Replacing config/layers.ts with generated configuration...")
                generated_config_file.rename(layer_config_file)
                self.logger.info("‚úÖ Updated main layer configuration")
                self.logger.info("‚ö†Ô∏è  IMPORTANT: config/layers.ts has been replaced with auto-generated configuration")
                self.logger.info("üìã To restore original: mv config/layers.ts.backup config/layers.ts")
            
            # Generate deployment summary including blob upload results
            blob_upload_info = {
                'blob_token_available': blob_uploader.blob_token is not None,
                'endpoints_uploaded': len(blob_uploader.uploaded_endpoints),
                'boundaries_uploaded': len(blob_uploader.uploaded_boundaries),
                'failed_endpoint_uploads': len(blob_uploader.failed_uploads),
                'failed_boundary_uploads': len(blob_uploader.failed_boundary_uploads),
                'uploaded_endpoints': blob_uploader.uploaded_endpoints,
                'uploaded_boundaries': blob_uploader.uploaded_boundaries,
                'failed_endpoints': blob_uploader.failed_uploads,
                'failed_boundaries': blob_uploader.failed_boundary_uploads
            }
            
            deployment_summary = {
                'project_name': self.project_name,
                'service_url': self.service_url,
                'pipeline_completed': datetime.now().isoformat(),
                'execution_time_minutes': (time.time() - self.start_time) / 60,
                'results_summary': {
                    'layers_discovered': len(self.results['service_analysis']['layers']),
                    'records_processed': self.results['extracted_data']['record_count'],
                    'fields_mapped': self.results['field_mappings']['mapped_field_count'],
                    'endpoints_generated': self.results['endpoints']['endpoint_count'],
                    'layers_configured': self.results['layer_configs']['layer_count']
                },
                'deployment_files': {
                    'endpoints_copied': copied_files,
                    'layer_config_updated': layer_config_file.exists(),
                    'ready_for_deployment': True
                },
                'blob_storage': blob_upload_info
            }
            
            # Save deployment summary
            summary_file = self.output_dir / "deployment_summary.json"
            with open(summary_file, 'w') as f:
                json.dump(deployment_summary, f, indent=2)
            
            # Generate final integration instructions
            microservice_info = self.results.get('microservice_deployment', {})
            integration_instructions = f"""# Final Integration Instructions

## üö® MICROSERVICE URL REQUIRED

Your microservice should now be deployed at:
**{microservice_info.get('microservice_url_placeholder', 'https://your-project-microservice.onrender.com')}**

## ‚òÅÔ∏è BLOB STORAGE STATUS

{f"‚úÖ **{len(blob_uploader.uploaded_endpoints)} endpoints uploaded** to Vercel Blob storage" if blob_uploader.blob_token and blob_uploader.uploaded_endpoints else "‚ö†Ô∏è  **Blob storage not configured** - endpoints available locally only"}
{f"üó∫Ô∏è  **{len(blob_uploader.uploaded_boundaries)} boundary files uploaded** to Vercel Blob storage" if blob_uploader.blob_token and blob_uploader.uploaded_boundaries else "üìç **No boundary files uploaded** - geographic visualizations will use local files"}
{f"‚ùå **{len(blob_uploader.failed_uploads)} endpoints failed upload** - check logs above" if blob_uploader.blob_token and blob_uploader.failed_uploads else ""}
{f"‚ùå **{len(blob_uploader.failed_boundary_uploads)} boundary files failed upload** - check logs above" if blob_uploader.blob_token and blob_uploader.failed_boundary_uploads else ""}
{f"üí° **Set BLOB_READ_WRITE_TOKEN** in .env.local to enable blob storage upload" if not blob_uploader.blob_token else "üîó **Large files automatically served** from blob storage"}

## Update Client Code

### 1. Environment Variables (Recommended)
```bash
# Add to your .env file
MICROSERVICE_URL={microservice_info.get('microservice_url_placeholder', 'https://your-project-microservice.onrender.com')}
{f"BLOB_READ_WRITE_TOKEN=your_token_here  # For blob storage uploads" if not blob_uploader.blob_token else "# Blob storage already configured ‚úÖ"}
```

### 2. Configuration File
```typescript
// Add to your config
export const MICROSERVICE_CONFIG = {{
  baseUrl: '{microservice_info.get('microservice_url_placeholder', 'https://your-project-microservice.onrender.com')}',
  timeout: 30000
}};
```

### 3. Update API Calls
Find any hardcoded microservice URLs in your code and replace with the new URL.

## Files Ready for Production
- ‚úÖ {copied_files} endpoint files deployed to public/data/endpoints/
- ‚úÖ Layer configuration updated in config/layers.ts
- ‚úÖ Microservice package created for deployment
- ‚ö†Ô∏è  **REQUIRED**: Add microservice URL to client code

## Test Your Setup
1. Verify microservice: `curl {microservice_info.get('microservice_url_placeholder', 'https://your-project-microservice.onrender.com')}/health`
2. Test endpoint loading in your application
3. Verify layer configurations are working

## Deployment Status
- **Microservice**: Manual deployment completed
- **Client Application**: Ready after URL configuration
- **Endpoints**: All 18 analysis endpoints deployed
- **Scoring**: All 15 algorithms applied
"""
            
            # Save integration instructions
            instructions_file = self.output_dir / "FINAL_INTEGRATION_INSTRUCTIONS.md"
            instructions_file.write_text(integration_instructions)
            
            self.logger.info(f"‚úÖ Phase 8 Complete: Deployed {copied_files} endpoints and updated configuration")
            self.logger.info("‚ö†Ô∏è  IMPORTANT: Microservice URL must be added to client code")
            self.logger.info(f"üìã Integration instructions: {instructions_file}")
            
            self.pipeline_state['phases_completed'].append('final_integration')
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase 8 Failed: {str(e)}")
            self.pipeline_state['phases_failed'].append('final_integration')
            return False
    
    async def _generate_final_report(self):
        """Generate comprehensive final report"""
        self.logger.info("\nüìã Generating Final Report...")
        
        elapsed_time = (time.time() - self.start_time) / 60
        
        # Calculate success metrics
        total_phases = 10  # Updated for Phase 7.5: Enhanced Layer Categorization
        completed_phases = len(self.pipeline_state['phases_completed'])
        success_rate = (completed_phases / total_phases) * 100
        
        report_content = f"""# Automation Pipeline Execution Report

## Project Information
- **Project Name**: {self.project_name}
- **Service URL**: {self.service_url}
- **Execution Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **Total Execution Time**: {elapsed_time:.1f} minutes

## Pipeline Results
- **Success Rate**: {success_rate:.1f}% ({completed_phases}/{total_phases} phases completed)
- **Phases Completed**: {', '.join(self.pipeline_state['phases_completed'])}
- **Phases Failed**: {', '.join(self.pipeline_state['phases_failed']) if self.pipeline_state['phases_failed'] else 'None'}

## Data Processing Summary
- **Layers Discovered**: {len(self.results['service_analysis']['layers']) if self.results['service_analysis'] else 'N/A'}
- **Records Processed**: {self.results['extracted_data']['record_count']:,} if self.results['extracted_data'] else 'N/A'
- **Fields Mapped**: {self.results['field_mappings']['mapped_field_count'] if self.results['field_mappings'] else 'N/A'}
- **Endpoints Generated**: {self.results['endpoints']['endpoint_count'] if self.results['endpoints'] else 'N/A'}
- **Layer Configurations**: {self.results['layer_configs']['layer_count'] if self.results['layer_configs'] else 'N/A'}

## Model Performance Summary
{self._format_performance_summary() if self.results.get('performance_report') else '- **Performance Report**: Not generated or failed'}

## Output Files
- **Project Directory**: `{self.output_dir}`
- **Service Analysis**: `service_analysis.json`
- **Merged Dataset**: `merged_dataset.csv`
- **Field Mappings**: `field_mappings.json`
- **Deployment Summary**: `deployment_summary.json`
- **Layer Configuration**: `config/layers.ts`
- **Performance Dashboard**: `MODEL_PERFORMANCE_DASHBOARD.html`
- **Performance Report**: `MODEL_PERFORMANCE_REPORT.json`
- **Model Traceability Report**: `MODEL_TRACEABILITY_REPORT.html`

## Model Traceability
{self._format_traceability_summary() if self.results.get('model_traceability') else '- **Model Traceability**: Report not generated'}

## Deployment Status
‚úÖ **Ready for Production Deployment**

The automation pipeline has successfully completed all phases and generated a production-ready microservice configuration.

## Next Steps
1. üß™ **Test the generated endpoints** in the development environment
2. üîç **Validate layer configurations** load correctly
3. üìä **Review generated scores** for accuracy
4. üöÄ **Deploy to staging** environment
5. ‚úÖ **Perform integration testing**
6. üéØ **Deploy to production**

---
*Generated by ArcGIS to Microservice Automation Pipeline v1.0*
"""
        
        # Save final report
        report_file = self.output_dir / "AUTOMATION_REPORT.md"
        report_file.write_text(report_content)
        
        self.logger.info(f"üìÑ Final report saved: {report_file}")
    
    def _format_performance_summary(self) -> str:
        """Format performance report summary for final report"""
        if not self.results.get('performance_report'):
            return "- **Performance Report**: Not available"
        
        perf_report = self.results['performance_report']
        
        # Extract key metrics
        best_model = perf_report.get('performance_summary', {}).get('best_individual_model', {})
        ensemble_perf = perf_report.get('performance_summary', {}).get('ensemble_performance', {})
        total_models = perf_report.get('project_info', {}).get('total_models', 0)
        
        # Count performance levels
        interpretations = perf_report.get('interpretations', {})
        excellent_count = sum(1 for interp in interpretations.values() if interp.get('performance_level') == 'EXCELLENT')
        good_count = sum(1 for interp in interpretations.values() if interp.get('performance_level') == 'GOOD')
        moderate_count = sum(1 for interp in interpretations.values() if interp.get('performance_level') == 'MODERATE')
        poor_count = sum(1 for interp in interpretations.values() if interp.get('performance_level') == 'POOR')
        
        summary = f"- **Total Models Trained**: {total_models}\n"
        summary += f"- **Best Individual Model**: {best_model.get('model_name', 'N/A')} (R¬≤ = {best_model.get('r2_score', 0):.3f})\n"
        
        if ensemble_perf:
            summary += f"- **Ensemble Performance**: R¬≤ = {ensemble_perf.get('r2_score', 0):.3f} ({ensemble_perf.get('performance_level', 'N/A')})\n"
        
        summary += f"- **Performance Distribution**: {excellent_count} Excellent, {good_count} Good, {moderate_count} Moderate, {poor_count} Poor\n"
        summary += f"- **Dashboard Available**: MODEL_PERFORMANCE_DASHBOARD.html\n"
        summary += f"- **Detailed Report**: MODEL_PERFORMANCE_REPORT.json"
        
        return summary
    
    def _format_traceability_summary(self) -> str:
        """Format model traceability summary for final report"""
        if not self.results.get('model_traceability'):
            return "- **Model Traceability**: Not available"
        
        trace_data = self.results['model_traceability']
        
        summary = f"- **Endpoints Analyzed**: {trace_data.get('endpoints_analyzed', 0)}\n"
        summary += f"- **Endpoints with Attribution**: {trace_data.get('endpoints_with_attribution', 0)}\n"
        
        attribution_coverage = 0
        if trace_data.get('endpoints_analyzed', 0) > 0:
            attribution_coverage = (trace_data.get('endpoints_with_attribution', 0) / trace_data['endpoints_analyzed']) * 100
        summary += f"- **Attribution Coverage**: {attribution_coverage:.1f}%\n"
        
        model_usage = trace_data.get('model_usage_summary', {})
        summary += f"- **Unique Models Used**: {len(model_usage)}\n"
        
        if model_usage:
            # Find most used model
            most_used_model = max(model_usage.items(), key=lambda x: len(x[1].get('endpoints_using', [])))
            summary += f"- **Most Used Model**: {most_used_model[0]} (used by {len(most_used_model[1]['endpoints_using'])} endpoints)\n"
        
        summary += f"- **Full Report**: MODEL_TRACEABILITY_REPORT.html"
        
        return summary
    
    async def _create_microservice_package(self, merged_path: Path, field_mappings: Dict, validation_results: Dict) -> Dict:
        """Create a complete microservice deployment package"""
        
        # Create microservice package directory
        package_dir = self.output_dir / "microservice_package"
        package_dir.mkdir(exist_ok=True)
        
        # Copy trained model files
        model_source_dir = Path("../shap-microservice/models")  # Adjust path as needed
        model_dest_dir = package_dir / "models"
        
        if model_source_dir.exists():
            import shutil
            shutil.copytree(model_source_dir, model_dest_dir, dirs_exist_ok=True)
            self.logger.info("üìã Copied trained model files")
        
        # Copy training data
        training_data_dest = package_dir / "data" 
        training_data_dest.mkdir(exist_ok=True)
        
        import shutil
        shutil.copy2(merged_path, training_data_dest / "training_data.csv")
        
        # Create deployment configuration
        deployment_config = {
            "project_name": self.project_name,
            "model_info": validation_results,
            "field_mappings": field_mappings,
            "service_url": self.service_url,
            "created_at": datetime.now().isoformat(),
            "deployment_instructions": {
                "render_service_name": f"{self.project_name}-microservice",
                "environment_variables": {
                    "MODEL_PATH": "/app/models",
                    "DATA_PATH": "/app/data"
                }
            }
        }
        
        config_file = package_dir / "deployment_config.json"
        with open(config_file, 'w') as f:
            json.dump(deployment_config, f, indent=2, default=str)
        
        # Create README for deployment
        readme_content = f"""# {self.project_name.title()} Microservice Deployment Package

## Quick Deploy to Render

1. **Create New Render Web Service**
   - Go to https://render.com/dashboard
   - Click "New" ‚Üí "Web Service"
   - Connect your microservice repository
   - Use these settings:
     - Name: `{self.project_name}-microservice`
     - Environment: `Python 3`
     - Build Command: `pip install -r requirements.txt`
     - Start Command: `python app.py`

2. **Upload This Package**
   - Copy contents of this package to your microservice repository
   - Commit and push to trigger deployment

3. **Get Your Microservice URL**
   - After deployment, copy the Render service URL
   - It will look like: `https://{self.project_name}-microservice.onrender.com`

4. **Update Client Configuration**
   - Add the microservice URL to your client application
   - Continue with the automation pipeline

## Package Contents
- `models/` - Trained XGBoost models with SHAP values
- `data/` - Training data and field mappings
- `deployment_config.json` - Deployment configuration
- This README with deployment instructions

## Model Performance
- R¬≤ Score: {validation_results.get('metrics', {}).get('r2_score', 'N/A')}
- RMSE: {validation_results.get('metrics', {}).get('rmse', 'N/A')}
- Features: {validation_results.get('metrics', {}).get('feature_count', 'N/A')}
"""
        
        readme_file = package_dir / "README.md"
        readme_file.write_text(readme_content)
        
        return {
            "package_path": str(package_dir),
            "config_file": str(config_file),
            "readme_file": str(readme_file),
            "render_service_name": f"{self.project_name}-microservice"
        }
    
    async def _pause_for_microservice_deployment(self, microservice_package: Dict):
        """Pause pipeline for manual microservice deployment"""
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info("üö® PIPELINE PAUSE: Manual Microservice Deployment Required")
        self.logger.info("=" * 80)
        self.logger.info("")
        self.logger.info("üì¶ Microservice package created at:")
        self.logger.info(f"   {microservice_package['package_path']}")
        self.logger.info("")
        self.logger.info("üöÄ NEXT STEPS (Manual):")
        self.logger.info("   1. Deploy microservice to Render using the package above")
        self.logger.info("   2. Get your new microservice URL from Render")
        self.logger.info("   3. Update your client code with the microservice URL")
        self.logger.info("")
        self.logger.info("üìã Deployment Instructions:")
        self.logger.info(f"   Package README: {microservice_package['readme_file']}")
        self.logger.info("   Complete Guide: scripts/automation/DEPLOYMENT_INSTRUCTIONS.md")
        self.logger.info("   Quick Guide: https://render.com/docs/deploy-python")
        self.logger.info("")
        self.logger.info("‚è∏Ô∏è  The automation will continue with endpoint generation...")
        self.logger.info("   (The remaining steps don't require the live microservice)")
        self.logger.info("")
        self.logger.info("üîÑ Continue? The pipeline will now generate endpoints and configurations...")
        self.logger.info("=" * 80)
        
        # Add microservice URL placeholder to results for later use
        self.results['microservice_deployment'] = {
            'package_created': True,
            'package_path': microservice_package['package_path'],
            'render_service_name': microservice_package['render_service_name'],
            'deployment_status': 'manual_deployment_required',
            'microservice_url_placeholder': f"https://{microservice_package['render_service_name']}.onrender.com"
        }
    
    async def _offer_cleanup(self):
        """Offer cleanup option after successful automation"""
        self.logger.info("\n" + "=" * 60)
        self.logger.info("üßπ CLEANUP RECOMMENDATION")
        self.logger.info("=" * 60)
        
        # Calculate approximate cleanup size from current project
        project_size = 0
        if self.output_dir.exists():
            for item in self.output_dir.rglob('*'):
                if item.is_file():
                    try:
                        project_size += item.stat().st_size
                    except:
                        pass
        
        def format_size(bytes):
            for unit in ['B', 'KB', 'MB', 'GB']:
                if bytes < 1024.0:
                    return f"{bytes:.1f} {unit}"
                bytes /= 1024.0
            return f"{bytes:.1f} TB"
        
        self.logger.info(f"üìä Current project size: {format_size(project_size)}")
        self.logger.info("üóÇÔ∏è  Intermediate files may accumulate over time")
        
        # Show blob storage info if available  
        blob_uploader = BlobUploader(self.project_root)
        if blob_uploader.blob_token and hasattr(blob_uploader, 'uploaded_endpoints'):
            if blob_uploader.uploaded_endpoints:
                self.logger.info(f"‚òÅÔ∏è  {len(blob_uploader.uploaded_endpoints)} endpoints uploaded to blob storage")
            if blob_uploader.failed_uploads:
                self.logger.info(f"‚ö†Ô∏è  {len(blob_uploader.failed_uploads)} endpoints failed blob upload")
        
        self.logger.info("")
        self.logger.info("üí° CLEANUP OPTIONS:")
        self.logger.info("   1. Run cleanup now (dry-run first):")
        self.logger.info("      python scripts/automation/cleanup_automation_artifacts.py --dry-run")
        self.logger.info("")
        self.logger.info("   2. Run cleanup with actual deletion:")
        self.logger.info("      python scripts/automation/cleanup_automation_artifacts.py")
        self.logger.info("")
        self.logger.info("   3. Aggressive cleanup (keep only 1 day):")
        self.logger.info("      python scripts/automation/cleanup_automation_artifacts.py --aggressive")
        self.logger.info("")
        self.logger.info("üìù What gets cleaned up:")
        self.logger.info("   ‚Ä¢ Old project directories (>7 days)")
        self.logger.info("   ‚Ä¢ Intermediate endpoint files")
        self.logger.info("   ‚Ä¢ Duplicate files in scoring directory")
        self.logger.info("   ‚Ä¢ Old backup files")
        self.logger.info("   ‚Ä¢ Temporary files (.pyc, __pycache__, etc.)")
        self.logger.info("   ‚Ä¢ Layer configuration backups")
        self.logger.info("")
        self.logger.info("‚ö†Ô∏è  IMPORTANT: Always run with --dry-run first to see what will be deleted")
        self.logger.info("=" * 60)


async def main():
    """Main function for command-line execution"""
    parser = argparse.ArgumentParser(
        description="Complete ArcGIS to Microservice Automation Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Basic usage
    python run_complete_automation.py "https://services8.arcgis.com/.../FeatureServer" --project nike_2025
    
    # With configuration file
    python run_complete_automation.py "SERVICE_URL" --project custom --config automation_config.json
        """
    )
    
    parser.add_argument("service_url", help="ArcGIS Feature Service URL")
    parser.add_argument("--project", default="automated_project", help="Project name")
    parser.add_argument("--config", help="Optional configuration file path")
    parser.add_argument("--target", default="MP10128A_B_P", 
                       help="Target variable for model training (default: MP10128A_B_P - Used H&R Block Online to Prepare Taxes)")
    
    args = parser.parse_args()
    
    # Load configuration if provided
    config = {}
    if args.config and Path(args.config).exists():
        with open(args.config) as f:
            config = json.load(f)
    
    # Add target variable to config
    config['target_variable'] = args.target
    
    # Initialize and run pipeline
    print(f"üöÄ Starting Complete Automation Pipeline")
    print(f"üìã Project: {args.project}")
    print(f"üåê Service: {args.service_url}")
    print(f"üéØ Target Variable: {args.target}")
    print("=" * 80)
    
    pipeline = CompleteAutomationPipeline(args.service_url, args.project, config)
    success = await pipeline.run_complete_pipeline()
    
    # Return appropriate exit code
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    # Run the async main function
    asyncio.run(main())