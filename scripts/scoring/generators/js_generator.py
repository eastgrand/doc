#!/usr/bin/env python3
"""
JavaScriptGenerator - Generate JavaScript scoring scripts from formulas

This module creates complete JavaScript scoring scripts based on 
data-driven mathematical formulas generated from SHAP importance analysis.
"""

import json
import textwrap
from pathlib import Path
from typing import Dict, List, Any, Optional
from formula_generator import DataDrivenFormulaGenerator

class JavaScriptScoringGenerator:
    """Generate complete JavaScript scoring scripts"""
    
    def __init__(self, project_root: str = None):
        """Initialize JavaScript generator"""
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.templates_dir = self.project_root / "scripts" / "scoring" / "generators" / "templates"
        
    def generate_scoring_script(self, analysis_type: str, formula_config: Dict[str, Any]) -> str:
        """Create complete .js scoring script file"""
        
        if not formula_config or 'components' not in formula_config:
            print(f"âŒ No formula config available for {analysis_type}")
            return ""
            
        print(f"ðŸ”§ Generating JavaScript for {analysis_type}...")
        
        # Load base template
        template = self._load_template()
        
        # Generate script components
        header_comment = self._generate_header_comment(analysis_type, formula_config)
        imports_section = self._generate_imports()
        data_loading = self._generate_data_loading(analysis_type)
        scoring_function = self._generate_scoring_function(analysis_type, formula_config)
        processing_loop = self._generate_processing_loop(analysis_type, formula_config)
        statistics_calculation = self._generate_statistics(analysis_type)
        output_section = self._generate_output(analysis_type)
        
        # Fill template
        script_content = template.format(
            header_comment=header_comment,
            imports_section=imports_section,
            data_loading=data_loading,
            scoring_function=scoring_function,
            processing_loop=processing_loop,
            statistics_calculation=statistics_calculation,
            output_section=output_section,
            analysis_type=analysis_type
        )
        
        print(f"âœ… Generated JavaScript scoring script for {analysis_type}")
        
        return script_content
    
    def _load_template(self) -> str:
        """Load scoring script template"""
        
        # Create a basic template structure
        template = '''/**
{header_comment}
 */

{imports_section}

{data_loading}

{scoring_function}

{processing_loop}

{statistics_calculation}

{output_section}
'''
        
        return template
    
    def _generate_header_comment(self, analysis_type: str, formula_config: Dict[str, Any]) -> str:
        """Generate header comment with formula documentation"""
        
        components = formula_config.get('components', [])
        formula = formula_config.get('formula', '')
        generation_method = formula_config.get('generation_method', 'data_driven')
        
        header = f"""
 * {analysis_type.title().replace('_', ' ')} Scoring Script - Data-Driven Algorithm
 * 
 * Generated using {generation_method} methodology from SHAP feature importance analysis
 * Formula: {formula}
 * 
 * Components ({len(components)} total):"""
        
        for i, component in enumerate(components[:5]):  # Show top 5 components
            weight = component.get('weight', 0)
            field_name = component.get('field_name', 'unknown')
            business_logic = component.get('business_logic', 'No description')
            
            header += f"""
 *   {i+1}. {field_name} (weight: {weight:.3f})
 *      {business_logic}"""
        
        if len(components) > 5:
            header += f"""
 *   ... and {len(components) - 5} more components"""
        
        header += f"""
 * 
 * Generated by: Data-Driven Scoring Algorithm Generator
 * Timestamp: {json.dumps({"timestamp": "2025-01-01T00:00:00Z"})}
"""
        
        return header
    
    def _generate_imports(self) -> str:
        """Generate imports section"""
        
        return """
const fs = require('fs');
const path = require('path');

console.log('ðŸŽ¯ Starting Data-Driven Scoring Analysis...');
"""
    
    def _generate_data_loading(self, analysis_type: str = None) -> str:
        """Generate data loading section with correct endpoint file"""
        
        # Map analysis types to their endpoint files (using hyphens not underscores)
        endpoint_filename = f"{analysis_type.replace('_', '-')}.json" if analysis_type else "correlation-analysis.json"
        
        return f"""
// Load the analysis data from endpoints
const dataPath = path.join(__dirname, '../../public/data/endpoints/{endpoint_filename}');
const analysisData = JSON.parse(fs.readFileSync(dataPath, 'utf8'));

if (!analysisData || !analysisData.results) {{
  console.error('âŒ No analysis dataset found');
  process.exit(1);
}}

console.log(`ðŸ“Š Processing ${{analysisData.results.length}} records for scoring...`);
"""
    
    def _generate_scoring_function(self, analysis_type: str, formula_config: Dict[str, Any]) -> str:
        """Generate the main scoring function"""
        
        components = formula_config.get('components', [])
        normalization_ranges = formula_config.get('normalization_ranges', {})
        
        function_name = f"calculate{analysis_type.title().replace('_', '')}Score"
        
        function_code = f"""
function {function_name}(record) {{
  // Data-driven scoring function generated from SHAP importance analysis
  
  let totalScore = 0;
  let componentCount = 0;
  
"""
        
        # Generate component calculations
        for component in components:
            field_name = component.get('field_name', 'unknown')
            original_field = component.get('original_field', field_name)
            weight = component.get('weight', 0)
            transformation = component.get('transformation', 'normalize_0_100')
            normalization = component.get('normalization_method', 'min_max_scale')
            
            # Get normalization range
            field_ranges = normalization_ranges.get(field_name, {'min': 0, 'max': 100})
            
            function_code += f"""
  // Component: {field_name} (weight: {weight:.3f})
  const {field_name}_raw = Number(record.{original_field}) || Number(record.{field_name}) || 0;
  let {field_name}_normalized = 0;
  
  if ({field_name}_raw > 0) {{
    // Normalization: {normalization}
"""
            
            if normalization == 'min_max_scale':
                function_code += f"""    {field_name}_normalized = Math.min(({field_name}_raw - {field_ranges['min']}) / ({field_ranges['max'] - field_ranges['min']}) * 100, 100);"""
            elif normalization == 'percentile_scale':
                function_code += f"""    {field_name}_normalized = Math.min(({field_name}_raw / {field_ranges['max']}) * 100, 100);"""
            elif normalization == 'already_normalized':
                function_code += f"""    {field_name}_normalized = Math.min({field_name}_raw, 100);"""
            else:
                function_code += f"""    {field_name}_normalized = Math.min(({field_name}_raw / {field_ranges['max']}) * 100, 100);"""
            
            # Apply transformation
            if transformation == 'log_scale':
                function_code += f"""
    
    // Transformation: log scale
    const {field_name}_transformed = Math.log({field_name}_normalized + 1) * (100 / Math.log(101));
    totalScore += {weight:.3f} * {field_name}_transformed;
"""
            elif transformation == 'inverse_scale':
                function_code += f"""
    
    // Transformation: inverse scale (gap/opportunity)
    const {field_name}_transformed = 100 - {field_name}_normalized;
    totalScore += {weight:.3f} * {field_name}_transformed;
"""
            else:
                function_code += f"""
    
    // Transformation: direct normalization
    totalScore += {weight:.3f} * {field_name}_normalized;
"""
            
            function_code += f"""
    componentCount++;
  }}
"""
        
        function_code += f"""
  
  // Ensure score is in 0-100 range
  const finalScore = Math.max(0, Math.min(100, totalScore));
  
  return Math.round(finalScore * 100) / 100;
}}
"""
        
        return function_code
    
    def _generate_processing_loop(self, analysis_type: str, formula_config: Dict[str, Any]) -> str:
        """Generate the record processing loop"""
        
        function_name = f"calculate{analysis_type.title().replace('_', '')}Score"
        score_field = f"{analysis_type}_score"
        
        return f"""
// Calculate scores for each record
let processedCount = 0;
const scoreStats = {{
  min: 100,
  max: 0,
  sum: 0,
  scores: []
}};

console.log('ðŸ”„ Calculating {analysis_type} scores using data-driven algorithm...');

analysisData.results.forEach((record, index) => {{
  const score = {function_name}(record);
  record.{score_field} = score;
  
  // Track statistics
  scoreStats.min = Math.min(scoreStats.min, score);
  scoreStats.max = Math.max(scoreStats.max, score);
  scoreStats.sum += score;
  scoreStats.scores.push(score);
  
  processedCount++;
  
  if (processedCount % 500 === 0) {{
    console.log(`   Processed ${{processedCount}}/${{analysisData.results.length}} records...`);
  }}
}});
"""
    
    def _generate_statistics(self, analysis_type: str) -> str:
        """Generate statistics calculation section"""
        
        return f"""
// Calculate final statistics
const avgScore = scoreStats.sum / processedCount;
scoreStats.scores.sort((a, b) => a - b);
const medianScore = scoreStats.scores[Math.floor(scoreStats.scores.length / 2)];

console.log('ðŸ“ˆ {analysis_type.title().replace("_", " ")} Scoring Statistics (Data-Driven):');
console.log(`   ðŸ“Š Records processed: ${{processedCount.toLocaleString()}}`);
console.log(`   ðŸ“Š Score range: ${{scoreStats.min.toFixed(1)}} - ${{scoreStats.max.toFixed(1)}}`);
console.log(`   ðŸ“Š Average score: ${{avgScore.toFixed(1)}}`);
console.log(`   ðŸ“Š Median score: ${{medianScore.toFixed(1)}}`);

// Show score distribution
const scoreRanges = {{
  'Exceptional Performance (90-100)': scoreStats.scores.filter(s => s >= 90).length,
  'High Performance (75-89)': scoreStats.scores.filter(s => s >= 75 && s < 90).length,
  'Good Performance (60-74)': scoreStats.scores.filter(s => s >= 60 && s < 75).length,
  'Moderate Performance (45-59)': scoreStats.scores.filter(s => s >= 45 && s < 60).length,
  'Limited Performance (0-44)': scoreStats.scores.filter(s => s < 45).length
}};

console.log('ðŸ“Š {analysis_type.title().replace("_", " ")} Distribution:');
Object.entries(scoreRanges).forEach(([range, count]) => {{
  const percentage = (count / processedCount * 100).toFixed(1);
  console.log(`   ${{range}}: ${{count.toLocaleString()}} (${{percentage}}%)`);
}});
"""
    
    def _generate_output(self, analysis_type: str) -> str:
        """Generate output and metadata section"""
        
        score_field = f"{analysis_type}_score"
        
        return f"""
// Show top performing records
const topPerformers = analysisData.results
  .sort((a, b) => b.{score_field} - a.{score_field})
  .slice(0, 15);

console.log('ðŸ† Top 15 {analysis_type.title().replace("_", " ")} Opportunities (Data-Driven):');
topPerformers.forEach((record, index) => {{
  console.log(`   ${{index + 1}}. ${{record.DESCRIPTION || record.ID}}: ${{record.{score_field}.toFixed(1)}} score`);
}});

// Add scoring metadata
analysisData.{analysis_type}_scoring_metadata = {{
  scoring_methodology: 'data_driven_shap_importance',
  algorithm_source: 'generated_from_feature_importance_analysis',
  generation_timestamp: new Date().toISOString(),
  score_statistics: {{
    mean: avgScore,
    median: medianScore,
    min: scoreStats.min,
    max: scoreStats.max,
    distribution: scoreRanges
  }},
  top_performers: topPerformers.slice(0, 10).map(record => ({{
    id: record.ID,
    name: record.DESCRIPTION,
    score: record.{score_field}
  }}))
}};

// Save updated data
console.log('ðŸ’¾ Saving updated dataset...');
fs.writeFileSync(dataPath, JSON.stringify(analysisData, null, 2));

console.log('âœ… {analysis_type.title().replace("_", " ")} scoring complete!');
console.log(`ðŸ“„ Updated dataset saved to: ${{dataPath}}`);
console.log(`ðŸŽ¯ All ${{processedCount.toLocaleString()}} records now include {score_field} field`);

console.log('\\nðŸ“‹ Next steps:');
console.log('   1. âœ… Created data-driven {score_field} for all records');
console.log('   2. âœ… Added scoring metadata and statistics');
console.log('   3. ðŸ”„ Update analysis processors to use new scores');
console.log('   4. ðŸ”„ Test analysis endpoint performance');
"""
    
    def generate_all_scripts(self, all_formulas: Dict[str, Dict[str, Any]]) -> Dict[str, str]:
        """Generate JavaScript scripts for all analysis types"""
        
        print(f"ðŸ”§ Generating JavaScript scripts for {len(all_formulas)} analysis types...")
        
        generated_scripts = {}
        
        for analysis_type, formula_config in all_formulas.items():
            script_content = self.generate_scoring_script(analysis_type, formula_config)
            
            if script_content:
                generated_scripts[analysis_type] = script_content
                
        print(f"âœ… Generated {len(generated_scripts)} JavaScript scoring scripts")
        
        return generated_scripts
    
    def save_scripts(self, generated_scripts: Dict[str, str], 
                    output_dir: str = None) -> List[str]:
        """Save generated scripts to files"""
        
        if not output_dir:
            output_dir = self.project_root / "scripts" / "scoring"
        else:
            output_dir = Path(output_dir)
            
        output_dir.mkdir(parents=True, exist_ok=True)
        
        saved_files = []
        
        for analysis_type, script_content in generated_scripts.items():
            filename = f"{analysis_type}-scores.js"
            file_path = output_dir / filename
            
            with open(file_path, 'w') as f:
                f.write(script_content)
                
            saved_files.append(str(file_path))
            print(f"ðŸ“„ Saved {filename}")
            
        print(f"âœ… Saved {len(saved_files)} JavaScript scoring scripts to {output_dir}")
        
        return saved_files

def main():
    """Test the JavaScriptGenerator"""
    
    print("ðŸš€ Testing JavaScriptGenerator...")
    
    generator = JavaScriptScoringGenerator()
    
    # Create test formula config
    test_formula_config = {
        'analysis_type': 'strategic',
        'formula': 'strategic_score = (0.350 Ã— market_component) + (0.300 Ã— competitive_component) + (0.200 Ã— demographic_component) + (0.150 Ã— economic_component)',
        'components': [
            {
                'field_name': 'market_share_field',
                'original_field': 'mp30034a_b_p',
                'weight': 0.350,
                'normalization_method': 'min_max_scale',
                'transformation': 'normalize_0_100',
                'business_logic': 'Market share indicates competitive position'
            },
            {
                'field_name': 'population_field',
                'original_field': 'total_population',
                'weight': 0.300,
                'normalization_method': 'min_max_scale',
                'transformation': 'log_scale',
                'business_logic': 'Population size indicates market potential'
            },
            {
                'field_name': 'income_field',
                'original_field': 'median_income',
                'weight': 0.200,
                'normalization_method': 'percentile_scale',
                'transformation': 'normalize_0_100',
                'business_logic': 'Income indicates purchasing power'
            }
        ],
        'normalization_ranges': {
            'market_share_field': {'min': 0, 'max': 50},
            'population_field': {'min': 0, 'max': 500000},
            'income_field': {'min': 20000, 'max': 200000}
        },
        'generation_method': 'data_driven_shap'
    }
    
    # Test single script generation
    print("\\n1. Testing single script generation...")
    script_content = generator.generate_scoring_script('strategic', test_formula_config)
    
    if script_content:
        print(f"âœ… Generated script ({len(script_content)} characters)")
        print(f"   Preview: {script_content[:200]}...")
        
        # Test script saving
        print("\\n2. Testing script saving...")
        test_scripts = {'strategic': script_content}
        saved_files = generator.save_scripts(test_scripts, 
                                           output_dir="scripts/scoring/generators/test_output")
        
        if saved_files:
            print(f"âœ… Saved {len(saved_files)} test scripts")
    
    print("\\nâœ… JavaScriptGenerator testing complete!")

if __name__ == "__main__":
    main()